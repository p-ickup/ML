{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "import googlemaps\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Initialize google maps client\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "gmaps_key = os.getenv(\"GOOGLE_MAPS_API_KEY\")\n",
    "gmaps = googlemaps.Client(key=gmaps_key)\n",
    "\n",
    "# Load in the dataset\n",
    "rides = pd.read_csv(\"RideShares.csv\")\n",
    "\n",
    "# We want to change Airport into a new column consisting of the coordinates of said airport\n",
    "## We can use Google Map's API called Geocoding API / GeoLocation API: https://github.com/googlemaps/google-maps-services-python\n",
    "\n",
    "def get_airport_coordinates(airport_name):\n",
    "    \"\"\"\n",
    "    This function will use OpenStreetMapAPI in order to query for the coordinates of an aiport \n",
    "    \n",
    "    input: airport name we will pass into the API\n",
    "    output: a tuple that holds both (lat, long)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(f'{airport_name} Airport')\n",
    "        if geocode_result: \n",
    "            location = geocode_result[0][\"geometry\"][\"location\"]\n",
    "            return location [\"lat\"], location[\"lng\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching coordinates for {airport_name}: {e}\")\n",
    "    return None, None\n",
    "    \n",
    "# append coordinates to new columns\n",
    "rides[['latitude', 'longitude']] = rides['Airport'].apply(lambda x: pd.Series(get_airport_coordinates(x)))\n",
    "\n",
    "# drop any rows with missing coordinates (if any)\n",
    "rides.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "    \n",
    "# Make this coordinates into a normalized range (Still be able to use Euclidian Distances)\n",
    "scaler = MinMaxScaler()\n",
    "rides[[\"latitude\", \"longitude\"]] = scaler.fit_transform(rides[[\"latitude\", \"longitude\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected Cell\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import googlemaps\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Initialize google maps client\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "gmaps_key = os.getenv(\"GOOGLE_MAPS_API_KEY\")\n",
    "gmaps = googlemaps.Client(key=gmaps_key)\n",
    "\n",
    "# Load in the dataset\n",
    "file_path = \"synthetic_customer_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# We want to change Airport into a new column consisting of the coordinates of said airport\n",
    "## We can use Google Map's API called Geocoding API / GeoLocation API: https://github.com/googlemaps/google-maps-services-python\n",
    "\n",
    "def get_airport_coordinates(airport_name):\n",
    "    \"\"\"\n",
    "    This function will use OpenStreetMapAPI in order to query for the coordinates of an aiport \n",
    "    \n",
    "    input: airport name we will pass into the API\n",
    "    output: a tuple that holds both (lat, long)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(f'{airport_name} Airport')\n",
    "        if geocode_result: \n",
    "            location = geocode_result[0][\"geometry\"][\"location\"]\n",
    "            return location [\"lat\"], location[\"lng\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching coordinates for {airport_name}: {e}\")\n",
    "    return None, None\n",
    "    \n",
    "# append coordinates to new columns\n",
    "df[['latitude', 'longitude']] = df['Airport'].apply(lambda x: pd.Series(get_airport_coordinates(x)))\n",
    "\n",
    "# drop any rows with missing coordinates (if any)\n",
    "df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "df.to_csv(file_path, index=False)\n",
    "    \n",
    "# Make this coordinates into a normalized range (Still be able to use Euclidian Distances)\n",
    "# scaler = MinMaxScaler()\n",
    "# df[[\"latitude\", \"longitude\"]] = scaler.fit_transform(df[[\"latitude\", \"longitude\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Load existing data\n",
    "df = pd.read_csv('RideShares.csv')\n",
    "df['FlightDate'] = pd.to_datetime(df['FlightDate'], errors='coerce', format='%m/%d/%Y')\n",
    "\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "fake.seed_instance(42)\n",
    "\n",
    "def generate_data():\n",
    "    \"\"\"\n",
    "    Generate synthetic data based on real dataset's distributions\n",
    "    \"\"\"\n",
    "\n",
    "#min time for range and max time for range (these will be given in time not time intervals)\n",
    "#prices might vary for ontario vs lax\n",
    "\n",
    "    # Use real airport choices\n",
    "    airport = random.choice(['LAX', 'ONT'])\n",
    "\n",
    "    #min_wait (in .25hr increments)\n",
    "    min_wait = random.choice([x / 60 for x in range(15, 121, 15)])\n",
    "\n",
    "    #max_wait\n",
    "    max_wait = random.choice([x / 60 for x in range(120, 301, 15)])\n",
    "\n",
    "    #max spending range\n",
    "    if random.random() < 0.75:  \n",
    "        max_spending_range = random.randrange(10, 41, 5)\n",
    "    else:  \n",
    "        max_spending_range = random.randrange(40, 101, 5)\n",
    "\n",
    "    # Bags within real observed range\n",
    "    def truncated_normal(mean, std, lower=0, upper=float('inf')):\n",
    "        a, b = (lower - mean) / std, (upper - mean) / std  \n",
    "        return truncnorm.rvs(a, b, loc=mean, scale=std)\n",
    "\n",
    "    mean_bag_number = df['BagNumber'].mean()\n",
    "    std_bag_number = df['BagNumber'].std()\n",
    "    bag_no = round(truncated_normal(mean_bag_number, std_bag_number))\n",
    "    \n",
    "\n",
    "    # Dropoff within real observed range\n",
    "    dropoff_range = fake.random_int(0, 10) / 10    \n",
    "\n",
    "    # Define date ranges for breaks\n",
    "    end_of_winter_break_start = datetime(2025, 1, 18)\n",
    "    end_of_winter_break_end = datetime(2025, 1, 20)\n",
    "    start_of_winter_break_start = datetime(2024, 12, 11)\n",
    "    start_of_winter_break_end = datetime(2024, 12, 14)\n",
    "    spring_break_start = datetime(2024, 3, 14)\n",
    "    spring_break_end = datetime(2024, 3, 23)\n",
    "    thanksgiving_start = datetime(2024, 11, 30)\n",
    "    thanksgiving_end = datetime(2024, 12, 1)\n",
    "\n",
    "    # Randomly choose one of these two periods\n",
    "    period_choice = random.choice([('spring_break', spring_break_start, spring_break_end),\n",
    "                                   ('end_of_winter_break', end_of_winter_break_start, end_of_winter_break_end),\n",
    "                                   ('start_of_winter_break',start_of_winter_break_start,start_of_winter_break_end),\n",
    "                                ('thanksgiving', thanksgiving_start, thanksgiving_end)])\n",
    "\n",
    "    # Sample a date based on the chosen period\n",
    "    period, start_date, end_date = period_choice\n",
    "    flight_date = fake.date_between_dates(start_date, end_date)\n",
    "\n",
    "    # Convert date to string in the desired format\n",
    "    flight_date_str = flight_date.strftime('%m/%d/%Y')  # Convert to string in the format \"%m/%d/%Y\"\n",
    "\n",
    "    customer = {\n",
    "        'FlightDate': flight_date_str,  # Use the formatted string here\n",
    "        'FlightTime': fake.time(),\n",
    "        'Airport': airport,\n",
    "        'MinWaitTime': min_wait,\n",
    "        'MaxWaitTime': max_wait,\n",
    "        'BagNumber': bag_no,\n",
    "        'MaxSpendingRange': max_spending_range,\n",
    "        'DropOffRange': dropoff_range\n",
    "    }\n",
    "\n",
    "    return customer\n",
    "\n",
    "# Generate synthetic customer data\n",
    "synthetic_customers = [generate_data() for i in range(10000)]\n",
    "\n",
    "# Write synthetic data to CSV\n",
    "with open('synthetic_customer_data.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['FlightDate', 'FlightTime', 'Airport', 'MinWaitTime', 'MaxWaitTime', 'BagNumber', 'MaxSpendingRange', 'DropOffRange']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for customer in synthetic_customers:\n",
    "        writer.writerow(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['latitude', 'longitude'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     df_normalized[columns_to_normalize] = scaler.fit_transform(df_normalized[columns_to_normalize])\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_normalized\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df2_normalized = \u001b[43mnormalize_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_normalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m df2_normalized.to_csv(\u001b[33m\"\u001b[39m\u001b[33mnormalized_synthetic_data.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mnormalize_dataframe\u001b[39m\u001b[34m(df2, columns_to_normalize)\u001b[39m\n\u001b[32m      8\u001b[39m df_normalized = df2.copy()\n\u001b[32m      9\u001b[39m scaler = MinMaxScaler()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df_normalized[columns_to_normalize] = scaler.fit_transform(\u001b[43mdf_normalized\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns_to_normalize\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_normalized\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xavier\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xavier\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xavier\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['latitude', 'longitude'] not in index\""
     ]
    }
   ],
   "source": [
    "#Normalizing num_bags, price_range, dropoff_distance to between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df2 = pd.read_csv('synthetic_customer_data.csv')\n",
    "columns_to_normalize = ['BagNumber', 'MaxSpendingRange','DropOffRange', 'latitude', 'longitude', 'MinWaitTime', 'MaxWaitTime']\n",
    "\n",
    "def normalize_dataframe(df2, columns_to_normalize):\n",
    "    df_normalized = df2.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized[columns_to_normalize] = scaler.fit_transform(df_normalized[columns_to_normalize])\n",
    "    return df_normalized\n",
    "\n",
    "df2_normalized = normalize_dataframe(df2, columns_to_normalize)\n",
    "df2_normalized.to_csv(\"normalized_synthetic_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical Encoding for Date & Time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"normalized_synthetic_data.csv\")\n",
    "\n",
    "# ensure date and time are in a consistent format \n",
    "# Date: MM/DD/YYYY\n",
    "# Time: HH:MM:SS\n",
    "df['FlightDate'] = pd.to_datetime(df['FlightDate'], errors='coerce', format='%m/%d/%Y')\n",
    "df['FlightTime'] = pd.to_datetime(df['FlightTime'], errors='coerce', format='%H:%M:%S').dt.time\n",
    "\n",
    "\n",
    "# combine date and time into one column that we can reference as a single variable (1/1/2025 13:00 )\n",
    "df['FlightDateTime'] = df['FlightDate'].astype(str) + \" \" + df['FlightTime'].astype(str)\n",
    "\n",
    "df[\"FlightDateTime\"] = pd.to_datetime(df[\"FlightDateTime\"]).dt.floor('min').dt.strftime(\"%m/%d/%Y %H:%M\")\n",
    "\n",
    "df['FlightDateTime'] = pd.to_datetime(df['FlightDateTime'], format=\"%m/%d/%Y %H:%M\")\n",
    "# Consider the earliest time in our data to serve as our base point (So Maybe January 1, 2024 will be 0)\n",
    "earliest_time = df['FlightDateTime'].min()\n",
    "\n",
    "# Also consider the latest time in ouur data \n",
    "latest_time = df['FlightDateTime'].max()\n",
    "\n",
    "# Calculate the number of minutes between a rows date/time to our base point \n",
    "df[\"ElapsedTime\"] = (((df['FlightDateTime'] - earliest_time).dt.total_seconds()) / 60).astype(int)\n",
    "\n",
    "# ### Stop here\n",
    "\n",
    "# Apply the cyclical encoding by mapping with sine and cosine for ciruclar representation where varying times across days will be seen \n",
    " # makes a 2D mapping of time where we can see the relationship between hours of a day and days in a year\n",
    "\n",
    "# total_minutes = (((latest_time - earliest_time).total_seconds()) / 60).astype(int)\n",
    "total_minutes = int(((latest_time - earliest_time).total_seconds()) / 60)\n",
    "\n",
    "\n",
    "df['datetime_sin'] = np.sin(2 * np.pi * df['ElapsedTime'] / total_minutes)\n",
    "\n",
    "df['datetime_cos'] = np.cos(2 * np.pi * df['ElapsedTime'] / total_minutes)\n",
    " \n",
    "df.to_csv(\"attempt1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
