{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgooglemaps\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "import googlemaps\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Initialize google maps client\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "gmaps_key = os.getenv(\"GOOGLE_MAPS_API_KEY\")\n",
    "gmaps = googlemaps.Client(key=gmaps_key)\n",
    "\n",
    "# Load in the dataset\n",
    "rides = pd.read_csv(\"RideShares.csv\")\n",
    "\n",
    "# We want to change Airport into a new column consisting of the coordinates of said airport\n",
    "## We can use Google Map's API called Geocoding API / GeoLocation API: https://github.com/googlemaps/google-maps-services-python\n",
    "\n",
    "def get_airport_coordinates(airport_name):\n",
    "    \"\"\"\n",
    "    This function will use OpenStreetMapAPI in order to query for the coordinates of an aiport \n",
    "    \n",
    "    input: airport name we will pass into the API\n",
    "    output: a tuple that holds both (lat, long)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(f'{airport_name} Airport')\n",
    "        if geocode_result: \n",
    "            location = geocode_result[0][\"geometry\"][\"location\"]\n",
    "            return location [\"lat\"], location[\"lng\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching coordinates for {airport_name}: {e}\")\n",
    "    return None, None\n",
    "    \n",
    "# append coordinates to new columns\n",
    "rides[['latitude', 'longitude']] = rides['Airport'].apply(lambda x: pd.Series(get_airport_coordinates(x)))\n",
    "\n",
    "# drop any rows with missing coordinates (if any)\n",
    "rides.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "    \n",
    "# Make this coordinates into a normalized range (Still be able to use Euclidian Distances)\n",
    "scaler = MinMaxScaler()\n",
    "rides[[\"latitude\", \"longitude\"]] = scaler.fit_transform(rides[[\"latitude\", \"longitude\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Load existing data\n",
    "df = pd.read_csv('RideShares.csv')\n",
    "df['FlightDate'] = pd.to_datetime(df['FlightDate'], errors='coerce', format='%m/%d/%Y')\n",
    "\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "fake.seed_instance(42)\n",
    "\n",
    "def generate_data():\n",
    "    \"\"\"\n",
    "    Generate synthetic data based on real dataset's distributions\n",
    "    \"\"\"\n",
    "\n",
    "#min time for range and max time for range (these will be given in time not time intervals)\n",
    "#prices might vary for ontario vs lax\n",
    "\n",
    "    # Use real airport choices\n",
    "    airport = random.choice(['LAX', 'ONT'])\n",
    "\n",
    "    #min_wait (in .25hr increments)\n",
    "    min_wait = random.choice([x / 60 for x in range(15, 121, 15)])\n",
    "\n",
    "    #max_wait\n",
    "    max_wait = random.choice([x / 60 for x in range(120, 301, 15)])\n",
    "\n",
    "    #max spending range\n",
    "    if random.random() < 0.75:  \n",
    "        max_spending_range = random.randrange(10, 41, 5)\n",
    "    else:  \n",
    "        max_spending_range = random.randrange(40, 101, 5)\n",
    "\n",
    "    # Bags within real observed range\n",
    "    def truncated_normal(mean, std, lower=0, upper=float('inf')):\n",
    "        a, b = (lower - mean) / std, (upper - mean) / std  \n",
    "        return truncnorm.rvs(a, b, loc=mean, scale=std)\n",
    "\n",
    "    mean_bag_number = df['BagNumber'].mean()\n",
    "    std_bag_number = df['BagNumber'].std()\n",
    "    bag_no = round(truncated_normal(mean_bag_number, std_bag_number))\n",
    "    \n",
    "\n",
    "    # Dropoff within real observed range\n",
    "    dropoff_range = fake.random_int(0, 10) / 10    \n",
    "\n",
    "    # Define date ranges for breaks\n",
    "    end_of_winter_break_start = datetime(2025, 1, 18)\n",
    "    end_of_winter_break_end = datetime(2025, 1, 20)\n",
    "    start_of_winter_break_start = datetime(2024, 12, 11)\n",
    "    start_of_winter_break_end = datetime(2024, 12, 14)\n",
    "    spring_break_start = datetime(2024, 3, 14)\n",
    "    spring_break_end = datetime(2024, 3, 23)\n",
    "    thanksgiving_start = datetime(2024, 11, 30)\n",
    "    thanksgiving_end = datetime(2024, 12, 1)\n",
    "\n",
    "    # Randomly choose one of these two periods\n",
    "    period_choice = random.choice([('spring_break', spring_break_start, spring_break_end),\n",
    "                                   ('end_of_winter_break', end_of_winter_break_start, end_of_winter_break_end),\n",
    "                                   ('start_of_winter_break',start_of_winter_break_start,start_of_winter_break_end),\n",
    "                                ('thanksgiving', thanksgiving_start, thanksgiving_end)])\n",
    "\n",
    "    # Sample a date based on the chosen period\n",
    "    period, start_date, end_date = period_choice\n",
    "    flight_date = fake.date_between_dates(start_date, end_date)\n",
    "\n",
    "    # Convert date to string in the desired format\n",
    "    flight_date_str = flight_date.strftime('%m/%d/%Y')  # Convert to string in the format \"%m/%d/%Y\"\n",
    "\n",
    "    customer = {\n",
    "        'FlightDate': flight_date_str,  # Use the formatted string here\n",
    "        'FlightTime': fake.time(),\n",
    "        'Airport': airport,\n",
    "        'MinWaitTime': min_wait,\n",
    "        'MaxWaitTime': max_wait,\n",
    "        'BagNumber': bag_no,\n",
    "        'MaxSpendingRange': max_spending_range,\n",
    "        'DropOffRange': dropoff_range\n",
    "    }\n",
    "\n",
    "    return customer\n",
    "\n",
    "# Generate synthetic customer data\n",
    "synthetic_customers = [generate_data() for i in range(10000)]\n",
    "\n",
    "# Write synthetic data to CSV\n",
    "with open('synthetic_customer_data.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['FlightDate', 'FlightTime', 'Airport', 'MinWaitTime', 'MaxWaitTime', 'BagNumber', 'MaxSpendingRange', 'DropOffRange']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for customer in synthetic_customers:\n",
    "        writer.writerow(customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing num_bags, price_range, dropoff_distance to between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df2 = pd.read_csv('attempt1.csv')\n",
    "columns_to_normalize = ['BagNumber', 'MaxSpendingRange','DropOffRange', 'latitude', 'longitude', 'MinWaitTime', 'MaxWaitTime', 'ElapsedTime', 'datetime_sin', 'datetime_cos']\n",
    "\n",
    "def normalize_dataframe(df2, columns_to_normalize):\n",
    "    df_normalized = df2.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler2 = StandardScaler()\n",
    "    df_normalized[columns_to_normalize] = scaler.fit_transform(df_normalized[columns_to_normalize])\n",
    "    df_normalized[columns_to_normalize] = scaler2.fit_transform(df_normalized[columns_to_normalize])\n",
    "    return df_normalized\n",
    "\n",
    "df2_normalized = normalize_dataframe(df2, columns_to_normalize)\n",
    "df2_normalized.to_csv(\"normalized_synthetic_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical Encoding for Date & Time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"normalized_synthetic_data.csv\")\n",
    "\n",
    "# ensure date and time are in a consistent format \n",
    "# Date: MM/DD/YYYY\n",
    "# Time: HH:MM:SS\n",
    "df['FlightDate'] = pd.to_datetime(df['FlightDate'], errors='coerce', format='%m/%d/%Y')\n",
    "df['FlightTime'] = pd.to_datetime(df['FlightTime'], errors='coerce', format='%H:%M:%S').dt.time\n",
    "\n",
    "\n",
    "# combine date and time into one column that we can reference as a single variable (1/1/2025 13:00 )\n",
    "df['FlightDateTime'] = df['FlightDate'].astype(str) + \" \" + df['FlightTime'].astype(str)\n",
    "\n",
    "df[\"FlightDateTime\"] = pd.to_datetime(df[\"FlightDateTime\"]).dt.floor('min').dt.strftime(\"%m/%d/%Y %H:%M\")\n",
    "\n",
    "df['FlightDateTime'] = pd.to_datetime(df['FlightDateTime'], format=\"%m/%d/%Y %H:%M\")\n",
    "# Consider the earliest time in our data to serve as our base point (So Maybe January 1, 2024 will be 0)\n",
    "earliest_time = df['FlightDateTime'].min()\n",
    "\n",
    "# Also consider the latest time in ouur data \n",
    "latest_time = df['FlightDateTime'].max()\n",
    "\n",
    "# Calculate the number of minutes between a rows date/time to our base point \n",
    "df[\"ElapsedTime\"] = (((df['FlightDateTime'] - earliest_time).dt.total_seconds()) / 60).astype(int)\n",
    "\n",
    "# ### Stop here\n",
    "\n",
    "# Apply the cyclical encoding by mapping with sine and cosine for ciruclar representation where varying times across days will be seen \n",
    " # makes a 2D mapping of time where we can see the relationship between hours of a day and days in a year\n",
    "\n",
    "# total_minutes = (((latest_time - earliest_time).total_seconds()) / 60).astype(int)\n",
    "total_minutes = int(((latest_time - earliest_time).total_seconds()) / 60)\n",
    "\n",
    "\n",
    "df['datetime_sin'] = np.sin(2 * np.pi * df['ElapsedTime'] / total_minutes)\n",
    "\n",
    "df['datetime_cos'] = np.cos(2 * np.pi * df['ElapsedTime'] / total_minutes)\n",
    " \n",
    "df.to_csv(\"attempt1.csv\", index=False)\n",
    "\n",
    "# Standardized data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "0    2569\n",
      "1    2491\n",
      "2    2474\n",
      "3    2466\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Assigning custom weights to each feature\n",
    "\n",
    "features = columns_to_normalize = [\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"datetime_sin\",\n",
    "    \"datetime_cos\",\n",
    "    \"ElapsedTime\",\n",
    "    \"MinWaitTime\",\n",
    "    \"MaxWaitTime\",\n",
    "    \"BagNumber\",\n",
    "    \"MaxSpendingRange\",\n",
    "    \"DropOffRange\"\n",
    "]\n",
    "\n",
    "feature_weights = np.array([3.0, 3.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "df = pd.read_csv(\"normalized_synthetic_data.csv\")\n",
    "df_weighted = pd.DataFrame()\n",
    "for i, feature in enumerate(features):\n",
    "    df_weighted[feature] = df[feature] * feature_weights[i]\n",
    "\n",
    "df_weighted.to_csv(\"normalized_synthetic_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
    "df_weighted['cluster'] = kmeans.fit_predict(df_weighted)\n",
    "\n",
    "# Save the clustered dataset\n",
    "df_weighted.to_csv(\"clustered_flight_data.csv\", index=False)\n",
    "\n",
    "# Optional: show cluster counts\n",
    "print(df_weighted['cluster'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
